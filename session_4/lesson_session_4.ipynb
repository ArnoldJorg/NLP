{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def build_markov_chain(text, order):\n",
    "    \"\"\"Create a Markov chain dictionary from input text ğŸ“–âœ¨\"\"\"\n",
    "    words = text.split()\n",
    "    markov_chain = {}\n",
    "\n",
    "    for i in range(len(words) - order):\n",
    "        key = tuple(words[i:i+order])\n",
    "        next_word = words[i+order]\n",
    "        \n",
    "        # the key value pairs are for example like this\n",
    "        # key : \"next word\"\n",
    "        if key not in markov_chain:\n",
    "            markov_chain[key] = []\n",
    "        markov_chain[key].append(next_word)\n",
    "        # print(\"this is the next word\", next_word)\n",
    "\n",
    "    return markov_chain\n",
    "\n",
    "# print(build_markov_chain(text=\"hello world hello world it is amazing\", order=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is happening above?\n",
    "1. A dictionary is made called markov_chain\n",
    "2. A for loop is run in which key value pairs are assigned to each word becomes a tuple (could be more than 1 depending on the order value e.g value = 2 would be 2 words in the tuple key)\n",
    "3. If checks to see if the key value pair already exists, in which case it would append the value on the right side of the existing key. (Remember all keys have to be unique in a dictionary)\n",
    "4. The right side of the key value pair is the next word from the string. \n",
    "5. The completed array is then the whole string broken down into Keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”® **Markov's Prophecy:**\n",
      "of time. Magic flows like a river, and shadows follow the wind. ğŸƒâœ¨ âœ¨\n"
     ]
    }
   ],
   "source": [
    "def generate_markov_text(chain, length=20, seed_word=None):\n",
    "    \"\"\"Generate a mystical prophecy using Markov magic! ğŸ”®\"\"\"\n",
    "    key = random.choice(list(chain.keys())) if not seed_word else seed_word\n",
    "    generated_words = list(key)\n",
    "\n",
    "    for _ in range(length):\n",
    "        if key in chain:\n",
    "            next_word = random.choice(chain[key])\n",
    "            generated_words.append(next_word)\n",
    "            key = tuple(generated_words[-len(key):])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# ğŸ“œ Behold, the sacred text of our AI prophecy:\n",
    "sacred_text = \"\"\"\n",
    "The moon ğŸŒ™ whispers to the sea ğŸŒŠ, while the stars ğŸŒŸ dance in the sky.\n",
    "The wizard speaks in riddles, and the trees ğŸŒ² sing the songs of time.\n",
    "Magic flows like a river, and shadows follow the wind. ğŸƒâœ¨\n",
    "\"\"\"\n",
    "\n",
    "# ğŸ§™â€â™‚ï¸ Markovâ€™s Spellbook\n",
    "markov_chain = build_markov_chain(sacred_text, order=2)\n",
    "generated_prophecy = generate_markov_text(markov_chain, length=15)\n",
    "\n",
    "print(\"ğŸ”® **Markov's Prophecy:**\")\n",
    "print(generated_prophecy + \" âœ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened here?\n",
    "1. A function is defined where a random key is picked from the Markov chain (unless a seed_word is given).\n",
    "2. This key is added to the generated_words list, converting it from a tuple into a list format.\n",
    "3. A loop runs for the desired number of repetitions, where:\n",
    "    â€¢ The function checks if the current key exists in the Markov chain.\n",
    "    â€¢ If it does, a random next word is picked from the list of possible next words.\n",
    "    â€¢ This word is added to generated_words, and the key is updated by shifting forward (keeping only the last order number of words).\n",
    "4. The result is a sentence where words are ordered in a way that commonly appears in the input text (while still allowing for random variations).\n",
    "5. Repetition of the same keys is possible, but only when the Markov chain allows for it naturally. The function doesn't enforce repetition but follows the probabilities from the training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnoldm./NLP/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/arnoldm./NLP/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.23.5 and <2.5.0 is required for this version of SciPy (detected version 1.23.0)\n",
      "  from scipy.sparse import csr_matrix, issparse\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ€ **RNN's Prophecy:**\n",
      "The enchanted forest hides secrets and great treasures.\n",
      "\n",
      "We have built the enchanted forest in four years from the ground up, so it is the perfect place to train for battle.\n",
      "\n",
      "No doubt, this is a journey that takes you to many ğŸŒŒ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# import numpy as np\n",
    "\n",
    "# ğŸš€ Load the AI wizard (GPT-2 model)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device=-1)  # Forces CPU usage\n",
    "\n",
    "\n",
    "# ğŸŒ™ Invoke RNN magic\n",
    "prompt = \"The enchanted forest hides secrets\"\n",
    "rnn_generated_text = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"\\nğŸŒ€ **RNN's Prophecy:**\")\n",
    "print(rnn_generated_text[0][\"generated_text\"] + \" ğŸŒŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤” Who Wins the Battle? YOU Decide! âš”ï¸âœ¨\n",
    "Questions for the Grand Wizard Council (YOU!):\n",
    "\n",
    "1ï¸âƒ£ Which text sounds more magical and poetic? âœ¨ğŸ“œ\n",
    "\n",
    "2ï¸âƒ£ Which AI wizard would you trust to write a fantasy novel? ğŸ“–ğŸ”¥\n",
    "\n",
    "3ï¸âƒ£ What happens if we change Markovâ€™s memory size (increase order)? ğŸ§ ğŸ”§\n",
    "\n",
    "4ï¸âƒ£ Which method do you think AI assistants like Siri use? ğŸ¤–ğŸ’¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RNN is more magical and poetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. RNN I would trust more for a fantasy novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”® **Markov's Prophecy:**\n",
      "of time. Magic flows like a river, and shadows follow the wind. ğŸƒâœ¨ âœ¨\n"
     ]
    }
   ],
   "source": [
    "# 3. Increae memory size - increase order:\n",
    "\n",
    "markov_chain = build_markov_chain(sacred_text, order=2)\n",
    "generated_prophecy = generate_markov_text(markov_chain, length=15)\n",
    "\n",
    "print(\"ğŸ”® **Markov's Prophecy:**\")\n",
    "print(generated_prophecy + \" âœ¨\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the order size therefore doesnt change much on small texts. On bigger texts however, it takes into account more previous words in order to predict the next coming word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. AI assistants like siri use multiple machine learning models for deep learrning. They might with some things use RNN's e.g speech recognition or flow of conversation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
