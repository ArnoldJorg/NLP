{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def build_markov_chain(text, order):\n",
    "    \"\"\"Create a Markov chain dictionary from input text 📖✨\"\"\"\n",
    "    words = text.split()\n",
    "    markov_chain = {}\n",
    "\n",
    "    for i in range(len(words) - order):\n",
    "        key = tuple(words[i:i+order])\n",
    "        next_word = words[i+order]\n",
    "        \n",
    "        # the key value pairs are for example like this\n",
    "        # key : \"next word\"\n",
    "        if key not in markov_chain:\n",
    "            markov_chain[key] = []\n",
    "        markov_chain[key].append(next_word)\n",
    "        # print(\"this is the next word\", next_word)\n",
    "\n",
    "    return markov_chain\n",
    "\n",
    "# print(build_markov_chain(text=\"hello world hello world it is amazing\", order=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is happening above?\n",
    "1. A dictionary is made called markov_chain\n",
    "2. A for loop is run in which key value pairs are assigned to each word becomes a tuple (could be more than 1 depending on the order value e.g value = 2 would be 2 words in the tuple key)\n",
    "3. If checks to see if the key value pair already exists, in which case it would append the value on the right side of the existing key. (Remember all keys have to be unique in a dictionary)\n",
    "4. The right side of the key value pair is the next word from the string. \n",
    "5. The completed array is then the whole string broken down into Keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 **Markov's Prophecy:**\n",
      "🌙 whispers to the sea 🌊, while the stars 🌟 dance in the sky. The wizard speaks ✨\n"
     ]
    }
   ],
   "source": [
    "def generate_markov_text(chain, length=20, seed_word=None):\n",
    "    \"\"\"Generate a mystical prophecy using Markov magic! 🔮\"\"\"\n",
    "    key = random.choice(list(chain.keys())) if not seed_word else seed_word\n",
    "    generated_words = list(key)\n",
    "\n",
    "    for _ in range(length):\n",
    "        if key in chain:\n",
    "            next_word = random.choice(chain[key])\n",
    "            generated_words.append(next_word)\n",
    "            key = tuple(generated_words[-len(key):])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# 📜 Behold, the sacred text of our AI prophecy:\n",
    "sacred_text = \"\"\"\n",
    "The moon 🌙 whispers to the sea 🌊, while the stars 🌟 dance in the sky.\n",
    "The wizard speaks in riddles, and the trees 🌲 sing the songs of time.\n",
    "Magic flows like a river, and shadows follow the wind. 🍃✨\n",
    "\"\"\"\n",
    "\n",
    "# 🧙‍♂️ Markov’s Spellbook\n",
    "markov_chain = build_markov_chain(sacred_text, order=2)\n",
    "generated_prophecy = generate_markov_text(markov_chain, length=15)\n",
    "\n",
    "print(\"🔮 **Markov's Prophecy:**\")\n",
    "print(generated_prophecy + \" ✨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened here?\n",
    "1. A function is defined where a random key is picked from the Markov chain (unless a seed_word is given).\n",
    "2. This key is added to the generated_words list, converting it from a tuple into a list format.\n",
    "3. A loop runs for the desired number of repetitions, where:\n",
    "    • The function checks if the current key exists in the Markov chain.\n",
    "    • If it does, a random next word is picked from the list of possible next words.\n",
    "    • This word is added to generated_words, and the key is updated by shifting forward (keeping only the last order number of words).\n",
    "4. The result is a sentence where words are ordered in a way that commonly appears in the input text (while still allowing for random variations).\n",
    "5. Repetition of the same keys is possible, but only when the Markov chain allows for it naturally. The function doesn't enforce repetition but follows the probabilities from the training text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (2.2.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: numpy in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torchvision) (2.2.3)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.1.0-cp311-cp311-macosx_10_10_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hDownloading torchaudio-2.2.2-cp311-cp311-macosx_10_13_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-11.1.0-cp311-cp311-macosx_10_10_x86_64.whl (3.2 MB)\n",
      "Installing collected packages: pillow, torchvision, torchaudio\n",
      "Successfully installed pillow-11.1.0 torchaudio-2.2.2 torchvision-0.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (2.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cached_path' from 'transformers' (/Users/arnoldm./NLP/venv/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cached_path, hf_bucket_url, CONFIG_NAME\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Clear the cache\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m file_utils\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'cached_path' from 'transformers' (/Users/arnoldm./NLP/venv/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import cached_path, hf_bucket_url, CONFIG_NAME\n",
    "\n",
    "# Clear the cache\n",
    "from transformers import file_utils\n",
    "file_utils.clear_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (2.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/arnoldm./NLP/venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy\n",
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 🌙 Invoke RNN magic\u001b[39;00m\n\u001b[32m      9\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mThe enchanted forest hides secrets\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m rnn_generated_text = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🌀 **RNN\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms Prophecy:**\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(rnn_generated_text[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m 🌌\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP/venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:287\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    286\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP/venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1368\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1360\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1361\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1362\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1365\u001b[39m         )\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP/venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1376\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1374\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m   1375\u001b[39m model_outputs = \u001b[38;5;28mself\u001b[39m.forward(model_inputs, **forward_params)\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/NLP/venv/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:434\u001b[39m, in \u001b[36mTextGenerationPipeline.postprocess\u001b[39m\u001b[34m(self, model_outputs, return_type, clean_up_tokenization_spaces, continue_final_message)\u001b[39m\n\u001b[32m    432\u001b[39m input_ids = model_outputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    433\u001b[39m prompt_text = model_outputs[\u001b[33m\"\u001b[39m\u001b[33mprompt_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m generated_sequence = \u001b[43mgenerated_sequence\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.tolist()\n\u001b[32m    435\u001b[39m records = []\n\u001b[32m    436\u001b[39m other_outputs = model_outputs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_outputs\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[31mRuntimeError\u001b[39m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# import numpy as np\n",
    "\n",
    "# 🚀 Load the AI wizard (GPT-2 model)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device=-1)  # Forces CPU usage\n",
    "\n",
    "\n",
    "# 🌙 Invoke RNN magic\n",
    "prompt = \"The enchanted forest hides secrets\"\n",
    "rnn_generated_text = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(\"\\n🌀 **RNN's Prophecy:**\")\n",
    "print(rnn_generated_text[0][\"generated_text\"] + \" 🌌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
